# 3.3 Base Model Selection

## 3.3.1 Overview of Base Model Selection Strategy

The selection of appropriate base models is a critical decision in the fine-tuning process, as it directly influences the model's performance, computational requirements, and applicability to specific domains. In this study, we employ a systematic approach to base model selection, considering multiple factors including model architecture, parameter scale, pre-training data characteristics, and computational efficiency.

Our base model selection strategy is guided by three primary principles: (1) **Architectural Compatibility** - ensuring the selected models are well-suited for the target tasks and can effectively leverage the fine-tuning techniques; (2) **Performance Baseline** - selecting models that demonstrate strong performance on relevant benchmarks; and (3) **Resource Efficiency** - balancing model capabilities with computational constraints.

## 3.3.2 Selected Base Models

### 3.3.2.1 Qwen2.5-7B-Instruct

**Model Architecture and Characteristics**
Qwen2.5-7B-Instruct is a 7-billion parameter instruction-tuned language model developed by Alibaba Cloud. The model is built upon the Qwen2.5 architecture, which incorporates several advanced techniques including:

- **SwiGLU Activation**: Utilizes SwiGLU activation functions in the feed-forward networks, which have demonstrated superior performance compared to traditional ReLU activations
- **RMSNorm**: Implements Root Mean Square Layer Normalization, providing more stable training and inference
- **Rotary Position Embedding (RoPE)**: Employs rotary position embeddings that effectively handle long sequences and maintain positional awareness
- **Group Query Attention (GQA)**: Implements grouped query attention mechanisms to reduce memory usage while maintaining attention quality

**Pre-training Data and Domain Coverage**
The model has been pre-trained on a diverse corpus of multilingual text, with particular emphasis on Chinese and English content. The pre-training data includes:
- Academic literature and research papers
- Web content and technical documentation
- Conversational data and instruction-response pairs
- Code and programming-related content

**Performance Characteristics**
Qwen2.5-7B-Instruct demonstrates strong performance across multiple benchmarks:
- **Mathematical Reasoning**: Achieves competitive scores on GSM8K and MATH datasets
- **Language Understanding**: Shows robust performance on C-Eval and CMMLU benchmarks
- **Instruction Following**: Excels in instruction-following tasks and conversational abilities
- **Code Generation**: Demonstrates strong capabilities in code generation and understanding

**Computational Requirements**
- **Memory Usage**: Approximately 14GB VRAM for inference, 28GB for training with LoRA
- **Training Efficiency**: Optimized for efficient fine-tuning with parameter-efficient methods
- **Inference Speed**: Balanced performance between speed and quality

### 3.3.2.2 GPT-OSS-20B

**Model Architecture and Characteristics**
GPT-OSS-20B is a 20-billion parameter open-source language model that represents a significant advancement in model scale and capability. The model architecture incorporates several key innovations:

- **Transformer Architecture**: Built upon the standard transformer architecture with enhanced attention mechanisms
- **Flash Attention**: Implements optimized attention computation for improved memory efficiency
- **Mixture of Experts (MoE)**: Utilizes sparse mixture of experts to increase model capacity while maintaining computational efficiency
- **Advanced Tokenization**: Employs sophisticated tokenization strategies for better handling of multilingual content

**Pre-training Data and Domain Coverage**
The model has been trained on an extensive corpus that includes:
- Multilingual text from diverse sources and domains
- Scientific and technical literature
- Code repositories and programming documentation
- Conversational and instructional data
- Psychological and medical content

**Performance Characteristics**
GPT-OSS-20B demonstrates exceptional performance across various domains:
- **Large-scale Reasoning**: Superior performance on complex reasoning tasks requiring extensive context
- **Multilingual Capabilities**: Strong performance across multiple languages, particularly in Chinese and English
- **Domain Expertise**: Deep understanding of specialized domains including psychology, medicine, and technology
- **Contextual Understanding**: Enhanced ability to maintain context over long conversations

**Computational Requirements**
- **Memory Usage**: Approximately 40GB VRAM for inference, 80GB for training with LoRA
- **Training Efficiency**: Requires significant computational resources but offers substantial performance gains
- **Inference Speed**: Slower than smaller models but provides higher quality outputs

### 3.3.2.3 InternLM2.5-7B-Chat

**Model Architecture and Characteristics**
InternLM2.5-7B-Chat is a 7-billion parameter conversational language model developed by Shanghai AI Laboratory. The model features several architectural innovations:

- **InternLM2.5 Architecture**: Enhanced transformer architecture with improved attention mechanisms
- **Chat-Optimized Design**: Specifically designed for conversational AI applications
- **Efficient Attention**: Implements optimized attention computation for better memory efficiency
- **Enhanced Position Encoding**: Advanced positional encoding strategies for improved sequence understanding

**Pre-training Data and Domain Coverage**
The model has been trained on a comprehensive dataset that includes:
- Extensive Chinese language content and conversations
- Multilingual dialogue data
- Professional and academic content
- Psychological counseling and mental health discussions
- Technical and domain-specific conversations

**Performance Characteristics**
InternLM2.5-7B-Chat excels in several key areas:
- **Chinese Language Proficiency**: Superior performance in Chinese language understanding and generation
- **Conversational Abilities**: Strong capabilities in maintaining context and generating coherent responses
- **Domain Adaptation**: Effective adaptation to specialized domains including psychology and counseling
- **Instruction Following**: Robust performance in following complex instructions and guidelines

**Computational Requirements**
- **Memory Usage**: Approximately 14GB VRAM for inference, 28GB for training with LoRA
- **Training Efficiency**: Optimized for conversational AI fine-tuning
- **Inference Speed**: Balanced performance suitable for real-time applications

## 3.3.3 Model Selection Rationale

### 3.3.3.1 Complementary Capabilities

The selection of these three models is based on their complementary strengths and capabilities:

**Scale Diversity**: The selection includes both 7B and 20B parameter models, allowing us to investigate the relationship between model scale and fine-tuning effectiveness. This diversity enables comprehensive analysis of how parameter count influences adaptation to psychological counseling tasks.

**Architectural Variety**: Each model employs different architectural innovations, providing insights into how various design choices affect fine-tuning performance and task adaptation.

**Domain Specialization**: The models exhibit different strengths in various domains, allowing us to assess their adaptability to psychological counseling contexts.

### 3.3.3.2 Computational Resource Considerations

**Efficient Resource Utilization**: The 7B models (Qwen2.5-7B-Instruct and InternLM2.5-7B-Chat) provide a balance between performance and computational efficiency, making them suitable for practical applications and rapid experimentation.

**High-Performance Options**: The 20B model (GPT-OSS-20B) offers superior performance for applications where computational resources are available and maximum quality is required.

**Scalability Analysis**: The combination of different model scales enables comprehensive analysis of the trade-offs between model size, performance, and resource requirements.

### 3.3.3.3 Task-Specific Suitability

**Psychological Domain Adaptation**: All three models demonstrate strong capabilities in understanding and generating human-like responses, which is crucial for psychological counseling applications.

**Multilingual Support**: The models provide robust support for both Chinese and English, essential for serving diverse user populations.

**Instruction Following**: Strong instruction-following capabilities ensure that the models can effectively implement psychological counseling protocols and guidelines.

## 3.3.4 Model Comparison and Analysis

### 3.3.4.1 Quantitative Comparison

| Model | Parameters | Architecture | Pre-training Data | Specialization | Memory (Inference) | Memory (Training) |
|-------|------------|--------------|-------------------|----------------|-------------------|-------------------|
| Qwen2.5-7B-Instruct | 7B | Qwen2.5 | Multilingual | General Purpose | 14GB | 28GB |
| GPT-OSS-20B | 20B | Transformer+MoE | Extensive Multilingual | Large-scale Reasoning | 40GB | 80GB |
| InternLM2.5-7B-Chat | 7B | InternLM2.5 | Chinese-focused | Conversational AI | 14GB | 28GB |

### 3.3.4.2 Qualitative Assessment

**Qwen2.5-7B-Instruct**
- **Strengths**: Balanced performance, efficient training, strong multilingual capabilities
- **Weaknesses**: Limited context window compared to larger models
- **Best Use Cases**: Rapid prototyping, resource-constrained environments, general-purpose applications

**GPT-OSS-20B**
- **Strengths**: Superior reasoning capabilities, extensive knowledge, high-quality outputs
- **Weaknesses**: High computational requirements, slower inference
- **Best Use Cases**: High-quality applications, complex reasoning tasks, research applications

**InternLM2.5-7B-Chat**
- **Strengths**: Excellent Chinese language proficiency, conversational optimization
- **Weaknesses**: Limited English performance, smaller parameter count
- **Best Use Cases**: Chinese-focused applications, conversational AI, rapid deployment

## 3.3.5 Expected Performance Characteristics

### 3.3.5.1 Fine-tuning Efficiency

**Parameter Efficiency**: The 7B models are expected to demonstrate higher parameter efficiency during fine-tuning, requiring fewer computational resources while achieving satisfactory performance improvements.

**Convergence Speed**: Smaller models typically converge faster during fine-tuning, enabling rapid experimentation and iteration.

**Memory Optimization**: The 7B models provide better memory efficiency, allowing for larger batch sizes and more extensive hyperparameter exploration.

### 3.3.5.2 Task-Specific Performance

**Psychological Counseling Tasks**: All models are expected to show significant improvements in psychological counseling capabilities after fine-tuning, with the 20B model potentially achieving the highest absolute performance.

**Domain Adaptation**: The models should demonstrate improved understanding of psychological concepts, counseling techniques, and therapeutic communication patterns.

**Response Quality**: Fine-tuned models are expected to generate more contextually appropriate, empathetic, and clinically relevant responses.

## 3.3.6 Conclusion

The selection of Qwen2.5-7B-Instruct, GPT-OSS-20B, and InternLM2.5-7B-Chat as base models provides a comprehensive foundation for investigating the effectiveness of fine-tuning approaches in psychological counseling applications. This diverse selection enables thorough analysis of how different architectural choices, parameter scales, and pre-training characteristics influence fine-tuning outcomes.

The combination of efficient 7B models and high-performance 20B models allows for practical deployment scenarios while maintaining the ability to achieve maximum performance when computational resources permit. This strategic selection supports our research objectives of developing effective, efficient, and scalable psychological counseling AI systems.
