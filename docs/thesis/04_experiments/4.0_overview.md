# 4. Experiments: Overview

## 4.1 Chapter Objectives
This chapter presents the experimental design, implementation, and results of fine-tuning large language models for psychological counseling. We systematically evaluate multiple base models, training strategies, and evaluation protocols to answer the following research questions:
- RQ1: How do different base models (7B vs 20B; Qwen vs GPT-OSS vs InternLM) compare after domain adaptation?
- RQ2: What is the effect of LoRA configurations and training hyperparameters on counseling performance?
- RQ3: How well do automated benchmark scores correlate with human-judged counseling quality, empathy, and safety?

## 4.2 Structure of the Chapter
- 4.1 Overview (this section)
- 4.2 Experimental Setup (hardware, software, environment variables)
- 4.3 Datasets (training/evaluation, preprocessing, splits)
- 4.4 Protocols (training protocols, evaluation pipelines, metrics)
- 4.5 Baselines (zero-shot, instruction models, prior works)
- 4.6 Main Results (benchmarks, psychology subsets, human eval)
- 4.7 Ablations (LoRA rank/alpha, batch, sequence length)
- 4.8 Safety & Ethics (content safety, bias, crisis handling)
- 4.9 Reproducibility (configs, seeds, scripts, checkpoints)

## 4.3 Experimental Scope
We focus on conversational psychological assistance scenarios requiring: contextual understanding, domain knowledge, empathy, safety compliance, and consistent guidance.

## 4.4 Summary of Model Settings
- Qwen2.5-7B-Instruct (7B): LoRA r=8, alpha=32, bfloat16; effective batch size 32
- GPT-OSS-20B (20B): LoRA r=16, alpha=64, bfloat16; effective batch size 32
- InternLM2.5-7B-Chat (7B): LoRA r=8, alpha=32, bfloat16; effective batch size 32

## 4.5 Evaluation Dimensions
- Automated: GSM8K, C-Eval, CMMLU, MMLU (psychology), MMLU-Pro, MMLU-Redux, Super-GPQA
- Human: counseling quality, empathy, therapeutic alliance, safety compliance
- Service vs Checkpoint: EvalScope service and checkpoint modes where applicable
