# 4.4 Protocols

## 4.4.1 Training Protocol
- Optimizer: AdamW (lr=1e-4), linear decay, warmup_ratio=0.1
- Precision: bfloat16
- LoRA: target all-linear; Qwen/InternLM r=8, alpha=32; GPT-OSS r=16, alpha=64
- Effective batch size: 32 (via per-device bs × grad accum)
- Length: 2048 (7B), 3072 (20B)
- Save/eval every 100 steps; keep 3 checkpoints

## 4.4.2 Evaluation Protocol
- Automated benchmarks via EvalScope (checkpoint mode)
- Psychology focus subsets configured via dataset-args
- Human evaluation on curated counseling prompts
- Safety screens applied before human review

## 4.4.3 Metrics
- Automated: AverageAccuracy (dataset default), macro-averaged per subset
- Human: Relevance (1–5), Professional Accuracy (1–5), Empathy (1–5), Safety (pass/fail)
- Service metrics (optional): latency (p50/p95), token throughput
