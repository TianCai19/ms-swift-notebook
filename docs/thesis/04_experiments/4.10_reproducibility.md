# 4.10 Reproducibility

## 4.10.1 Training Commands (examples)
```bash
# Qwen2.5-7B SFT (LoRA)
CUDA_VISIBLE_DEVICES=6 \
swift sft \
  --model Qwen/Qwen2.5-7B-Instruct \
  --train_type lora \
  --dataset CodyWhy/mh-sharegpt-20250820 \
  --torch_dtype bfloat16 \
  --num_train_epochs 1 \
  --per_device_train_batch_size 8 \
  --per_device_eval_batch_size 4 \
  --router_aux_loss_coef 1e-3 \
  --learning_rate 1e-4 \
  --lora_rank 8 \
  --lora_alpha 32 \
  --target_modules all-linear \
  --gradient_accumulation_steps 4 \
  --eval_steps 100 \
  --save_steps 100 \
  --save_total_limit 3 \
  --logging_steps 20 \
  --max_length 3072 \
  --output_dir ./output/qwen2.5-7b-sft/v1-$(date +%Y%m%d-%H%M%S) \
  --warmup_ratio 0.1 \
  --dataloader_num_workers 8 \
  --packing true \
  --attn_impl flash_attn \
  --gradient_checkpointing true \
  --report_to swanlab \
  --swanlab_token $SWANLAB_TOKEN \
  --swanlab_project $SWANLAB_PROJECT \
  --swanlab_mode cloud \
  --swanlab_exp_name qwen2.5-7b-sft-v1
```

## 4.10.2 Evaluation Commands (checkpoint)
```bash
./temp/eval_multipy.sh
./temp/eval_quick.sh
```

## 4.10.3 Versioning
- Output dirs: `./output/<model>-sft/v1-<date-time>`
- SwanLab exp names: `<model>-sft-v1`
- Seeds: 42 across runs
