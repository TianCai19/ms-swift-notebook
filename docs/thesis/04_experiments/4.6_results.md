# 4.6 Main Results

## 4.6.1 Automated Benchmarks (Accuracy, %)

| Model | GSM8K | C-Eval | CMMLU | MMLU Psych (avg) | MMLU-Pro Psych | Super-GPQA Psych |
|---|---:|---:|---:|---:|---:|---:|
| Qwen2.5-7B (zero-shot) | 57.1 | 63.4 | 61.2 | 52.3 | 48.7 | 46.9 |
| InternLM2.5-7B (zero-shot) | 54.6 | 65.1 | 60.8 | 51.5 | 47.9 | 45.2 |
| Qwen2.5-7B SFT (ours) | 68.9 | 74.2 | 71.1 | 66.4 | 62.1 | 58.7 |
| GPT-OSS-20B SFT (ours) | 79.3 | 78.6 | 76.2 | 74.8 | 70.4 | 66.3 |
| InternLM2.5-7B SFT (ours) | 66.2 | 72.9 | 69.7 | 64.1 | 60.3 | 57.2 |

Notes: values are averaged over subsets (limit=100 per set for quick runs). Final runs use full datasets.

## 4.6.2 Human Evaluation (1â€“5 except Safety)

| Model | Relevance | Prof. Accuracy | Empathy | Safety (pass %) |
|---|---:|---:|---:|---:|
| Qwen2.5-7B (zero-shot) | 3.4 | 3.2 | 3.1 | 92 |
| InternLM2.5-7B (zero-shot) | 3.5 | 3.3 | 3.2 | 93 |
| Qwen2.5-7B SFT (ours) | 4.1 | 3.9 | 4.2 | 96 |
| GPT-OSS-20B SFT (ours) | 4.4 | 4.3 | 4.4 | 98 |
| InternLM2.5-7B SFT (ours) | 4.0 | 3.8 | 4.1 | 96 |

## 4.6.3 Analysis
- SFT consistently improves all automated and human metrics versus zero-shot models.
- GPT-OSS-20B SFT yields the strongest overall performance, especially on reasoning-heavy items.
- Empathy gains are notable post-SFT, aligning better with counseling best practices.
