# 4.7 Ablation Studies

## 4.7.1 LoRA Rank and Alpha
- r ∈ {4, 8, 16}, alpha = 4r. Peak accuracy at r=8 for 7B; r=16 for 20B.

## 4.7.2 Batch Size and Accumulation
- Effective batch 16 vs 32 vs 64: 32 balances stability and throughput; 64 shows diminishing returns.

## 4.7.3 Sequence Length
- 1024 vs 2048 vs 3072: psychology dialogues benefit from 2048+; 7B peaks near 2048; 20B benefits from 3072.

## 4.7.4 Attention Implementation
- flash_attn vs sdpa: flash_attn improves throughput 20–35% with comparable or better accuracy.
