# 模型评测与心理学基准

## 概述
记录 MS-Swift 模型评测阶段的详细流程、心理学相关基准测试方法，以及评测工具的使用和问题解决。

## 1. 评测阶段到底发生了什么

### 1.1 EvalScope 评测流程

EvalScope 是 ModelScope 团队打造的一站式模型评估框架，与 ms-swift 深度集成。主要流程如下：

**输入层**：
- 自动加载指定的模型来源（ModelScope 模型ID 或本地路径）
- 加载指定的评测数据集（如 C-Eval、CMMLU 等）

**核心功能**：
- 使用不同后端（Native、OpenCompass、RAG、ThirdParty）统一跑多个数据集
- 提供"服务评测"(service) 和"checkpoint 本地评测"(checkpoint) 两种模式

**输出结果**：
- 准确率（Accuracy）、RMSE、合规性评分
- 性能指标（吞吐/延迟）、资源占用
- 可视化对比结果

**总结**：EvalScope 是自动加载模型 & 数据 → 发起推理 → 对比参考标准答案 → 输出评测结果 & 性能指标的标准流程。

### 1.2 评测实际过程示例

当你运行命令：
```bash
evalscope eval \
  --model mh-sft-7b \
  --api-url http://127.0.0.1:8000/v1 \
  --datasets ceval cmmlu \
  --limit 100
```

**背后流程**：
1. EvalScope 启动后调用你的本地 model 服务
2. 发出约 limit 条问题请求
3. 模型生成答案后，EvalScope 将其对照基准答案计算准确率
4. 最终生成报告，包含整体准确率，按学科分类的子集统计（如心理学）
5. 如果用 Judge 模型，则用另一个模型判断回应质量，算出同理评分

## 2. 心理学相关基准详解

### 2.1 PsyQA (Sun et al., 2021)

**来源**：中文心理健康问答数据集，来自专业平台
**规模**：约 22K 问题 + 56K 长回答
**特点**：部分答案标注了心理辅导策略（Interpretation、Direct Guidance 等）
**用途**：测试同理心、支持性回答的质量和策略覆盖情况
**评价指标**：准确率、BLEU/ROUGE

### 2.2 C-Eval

**特点**：全学科中文评测集，覆盖从中学到职业的 52 个学科
**难度分级**：中学、高中、大学、职业阶段
**特殊子集**：C-Eval Hard（更难的子集）
**用途**：测试模型在"知识+推理"方面的真实能力
**心理学应用**：心理学作为子学科可单独统计

### 2.3 CMMLU

**特点**：Chinese MMLU 中文版多学科任务套件
**内容**：类似 C-Eval，可用于测检模型多领域泛化能力
**用途**：多学科知识评测

### 2.4 CPsyExam (Zhao et al., 2025)

**来源**：由高校构建的心理学多任务基准
**规模**：4,102 道题，覆盖 39 个心理学子领域
**任务类型**：
- 知识型多选（MCQA）
- 案例分析（CA）
- 问答针对心理学学科考试题设计
**特点**：支持知识与案例推理双向评测
**指标**：零样本、五样本准确率对比

### 2.5 CounselBench (2025)

**特点**：临床取向的心理咨询评估框架
**特殊功能**：含对抗集 CounselBench-Adv，聚焦高风险&失误模式
**用途**：更临床化，能测安全性与失误模式

### 2.6 HealthBench (2025)

**特点**：面向医疗健康领域的综合评测
**特殊功能**：含安全维度，可借其"安全/有害性"范式做参考
**用途**：医疗健康综合评测

### 2.7 PCEB (Psychological Counselor Examination Benchmark)

**来源**：中国国家心理咨询师考试题库
**题型**：选择题 + 案例分析
**测试点**：模型是否具备专业知识和案例推理能力

**指标**：
- SMCQ/MMCQ → 单选/多选的准确率（标准 + 弹性）
- Case QA → 案例分析，ROUGE/BLEU

### 2.8 PsyDT (Xie et al., 2025)

**用途**：用于测试咨询任务的同理心与安全性

**指标**：
- 情绪共情 (EmoE)
- 认知共情 (CogE)
- 对话策略 (Con.)
- 态度与状态 (Sta.)
- 安全性 (Saf.)

## 3. 评测工具使用方法

### 3.1 方案 A：本地 OpenAI 兼容服务 + lm-eval-harness

**适用场景**：你已经用 swift serve/swift openai 把模型暴露成 OpenAI chat/completions 接口

**安装评测工具**：
```bash
pip install "lm-eval==0.4.2"
```

**准备数据**：
创建 `data/pceb_mcq.jsonl`，每行一题：
```json
{"id":"q1","question":"求助者因出国与家庭矛盾产生的心理冲突类型是？","choices":["变形","趋避式","常形","双趋式"],"answer":"B"}
{"id":"q2","question":"……","choices":["A","B","C","D"],"answer":"D"}
```

**定义任务**：
创建 `tasks/pceb_mcq.yaml`：
```yaml
task: pceb_mcq
dataset_path: data/pceb_mcq.jsonl
output_type: multiple_choice
doc_to_text: "问：{{question}}\n选项：{% for c in choices %}{{ loop.index0 | chr(65) }}. {{ c }} {% endfor %}\n答："
doc_to_target: "{{answer}}"
doc_to_choice: "{{choices}}"
should_decontaminate: false
fewshot_delimiter: "\n\n"
generation_kwargs:
  temperature: 0.0
```

**运行评测**：
```bash
lm_eval \
  --model openai-chat-completions \
  --model_args api_base=http://127.0.0.1:8000/v1,model=mh-sft-7b \
  --tasks pceb_mcq \
  --num_fewshot 0 \
  --batch_size 1
```

### 3.2 方案 B：离线直读权重

**适用场景**：直接测 `./export/mh-sft-qwen2p5-7b`，不依赖服务

**安装**：
```bash
pip install "lm-eval==0.4.2" transformers accelerate
```

**运行评测**：
```bash
lm_eval \
  --model hf \
  --model_args pretrained=./export/mh-sft-qwen2p5-7b,dtype=bfloat16,attn_implementation=flash_attention_2 \
  --tasks pceb_mcq \
  --num_fewshot 0 \
  --batch_size 1
```

**注意**：如果 flash-attn 已装好，`attn_implementation` 设成 `flash_attention_2` 会更快；否则去掉它用默认 `sdpa` 也能跑。

## 4. 评测指标解释

### 4.1 选择题评测

**标准准确率（exact match）**：多选需完全一致才记对
**弹性准确率（subset/overlap）**：给部分分，更贴合多选现实打分

### 4.2 主观题评测

**文本指标**：Rouge-1 / Rouge-L / BLEU 等
**要点命中率**：粗评，适合简答要点匹配的快速打分

### 4.3 心理学专用维度

**专业维度**：
- 情绪共情
- 认知共情
- 对话策略
- 态度与状态
- 安全性

**评分方式**：可先让大模型做自动初评，再抽样人工复核

## 5. 题库准备建议

### 5.1 推荐题库

**PCEB/CPsyExam**：
- 最适合起步做"知识/伦理/案例"综合评测
- PCEB 兼有 SMCQ/MMCQ 与案例题
- CPsyExam 覆盖 30+ 子领域，零样本/五样本都能测

**临床/干预流程**：
- 如果涉及 PM+ 或 CBT 的阶段化流程
- 可在案例题里加入"阶段识别/策略建议"要点表
- 做要点打分

### 5.2 数据格式

**选择题格式**：
```json
{
  "id": "unique_id",
  "question": "问题描述",
  "choices": ["选项A", "选项B", "选项C", "选项D"],
  "answer": "B"
}
```

**多选题格式**：
```json
{
  "id": "unique_id",
  "question": "问题描述",
  "choices": ["选项A", "选项B", "选项C", "选项D"],
  "answer": "B,C"
}
```

## 6. 常见问题与解决方案

### 6.1 EvalScope 卡住的原因

**代理问题**：evalscope/worker 如果没继承到 NO_PROXY 设置，会把本地 127.0.0.1:8000 也走代理

**数据下载**：首次评测会下数据，离线环境需要配置镜像

**流式/超时**：部分评测器默认开流式或超短超时

### 6.2 解决方案

**环境变量设置**：
```bash
export NO_PROXY=127.0.0.1,localhost,::1
export no_proxy=$NO_PROXY
```

**离线数据**：优先用本地 JSONL 评测以避免下载

**参数调整**：确保关闭 stream 和把 timeout 调大

## 7. 后续发展方向

### 7.1 自动化评分

**多维度评分**：知识/案例/共情/安全的雷达图
**要点匹配**：自动识别关键信息点
**安全检测**：高风险内容的自动识别

### 7.2 论文支持

**对比表格**：不同模型版本的性能对比
**可视化**：性能趋势图、错误分析
**复现性**：完整的实验记录和配置

## 8. 总结

通过本次评测实践，我们验证了：

1. ✅ 模型可以正常加载和推理
2. ✅ 多种评测工具都能正常工作
3. ✅ 心理学基准测试方法已掌握
4. ✅ 评测流程和指标理解清晰

**推荐配置**：
- 快速测试：使用 lm-eval-harness + 本地服务
- 完整评测：使用 EvalScope + 多种基准
- 专业分析：结合人工评估和自动评分

## 9. 变更日志

- 2025-08-20: 初始版本，记录评测流程和心理学基准详解
- 2025-08-20: 添加评测工具使用方法和问题解决方案
