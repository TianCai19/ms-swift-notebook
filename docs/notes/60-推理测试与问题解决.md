# 推理测试与问题解决

## 概述
记录 MS-Swift 模型推理测试过程中遇到的问题、解决方案和最佳实践。

## 1. 推理测试环境

**测试时间**: 2025-08-20  
**硬件环境**: NVIDIA H100 GPU (卡号 6)  
**模型**: Qwen/Qwen2.5-7B-Instruct + LoRA checkpoint-508  
**检查点路径**: `./output/mh-sft/v0-20250820-144506/checkpoint-508`

## 2. 遇到的问题与解决方案

### 2.1 参数解析错误

**问题描述**:
```bash
swift infer \
  --ckpt_dir ./output/mh-sft/v0-20250820-144506/checkpoint-508 \
  --max_new_tokens 512 \
  --prompt "写一段小故事，主角是一只喜欢写代码的猫"
```

**错误信息**:
```
ValueError: remaining_argv: ['--prompt', '写一段小故事，主角是一只喜欢写代码的猫']
```

**原因分析**: 
- `--prompt` 参数在当前版本的 MS-Swift 中不被支持
- 参数解析器将不认识的参数当作"剩余未解析参数"抛出错误

**解决方案**:
1. **交互式推理**（推荐）:
```bash
CUDA_VISIBLE_DEVICES=6 \
swift infer \
  --model Qwen/Qwen2.5-7B-Instruct \
  --adapters ./output/mh-sft/v0-20250820-144506/checkpoint-508 \
  --infer_backend pt \
  --attn_impl flash_attn \
  --stream true \
  --max_new_tokens 512
```

2. **使用 Python API**:
```python
from swift.llm import PtEngine, RequestConfig, InferRequest

engine = PtEngine(
    "Qwen/Qwen2.5-7B-Instruct", 
    adapters=["./output/mh-sft/v0-20250820-144506/checkpoint-508"]
)
req = [InferRequest(messages=[{'role':'user','content':'你好，请用中文简单介绍一下你是一个什么模型？'}])]
resp = engine.infer(req, RequestConfig(max_tokens=512, temperature=0.7))
print(resp[0].choices[0].message.content)
```

### 2.2 设备指定参数错误

**问题描述**:
```bash
swift infer \
  --ckpt_dir ./output/mh-sft/v0-20250820-144506/checkpoint-508 \
  --device cuda:6 \
  --max_new_tokens 512
```

**错误信息**:
```
infer.py: 错误：选项不明确：--device 可能匹配 --device_map, --device-map
```

**原因分析**: 
- `--device` 参数不存在，会被误认为 `--device_map` 或 `--device-map`
- MS-Swift 使用 `--device_map` 来指定设备

**解决方案**:
1. **使用环境变量**（推荐）:
```bash
CUDA_VISIBLE_DEVICES=6 \
swift infer \
  --model Qwen/Qwen2.5-7B-Instruct \
  --adapters ./output/mh-sft/v0-20250820-144506/checkpoint-508 \
  --infer_backend pt \
  --attn_impl flash_attn \
  --stream true \
  --max_new_tokens 512
```

2. **使用 device_map 参数**:
```bash
swift infer \
  --model Qwen/Qwen2.5-7B-Instruct \
  --adapters ./output/mh-sft/v0-20250820-144506/checkpoint-508 \
  --device_map cuda:6 \
  --infer_backend pt \
  --attn_impl flash_attn \
  --stream true \
  --max_new_tokens 512
```

### 2.3 中文显示乱码问题

**问题描述**: 在 zsh 终端中输入和显示中文时出现乱码

**错误信息**:
```bash
locale: 无法将 LC_CTYPE 设置为默认区域设置：没有此文件或目录
locale: 无法将 LC_MESSAGES 设置为默认区域设置：没有此文件或目录
locale: 无法将 LC_ALL 设置为默认区域设置：没有此文件或目录
```

**原因分析**: 
- 系统 locale 配置不完整
- 缺少对应的 UTF-8 本地化包

**解决方案**:
```bash
# 1. 安装 locales 包
sudo apt-get update
sudo apt-get install -y locales

# 2. 生成中文和英文 locale
sudo locale-gen en_US.UTF-8 zh_CN.UTF-8

# 3. 设置默认 locale
sudo update-locale LANG=en_US.UTF-8 LC_ALL=en_US.UTF-8

# 4. 重新登录终端或执行
exec $SHELL -l
```

**验证修复**:
```bash
locale
echo "你好，世界"
```

## 3. 推理后端对比

### 3.1 PyTorch 后端 (pt)

**特点**:
- ✅ 启动速度最快
- ✅ 零额外依赖
- ✅ 适合快速测试和调试

**适用场景**: 单机调试、快速冒烟测试、开发阶段验证

**命令示例**:
```bash
CUDA_VISIBLE_DEVICES=6 \
swift infer \
  --model Qwen/Qwen2.5-7B-Instruct \
  --adapters ./output/mh-sft/v0-20250820-144506/checkpoint-508 \
  --infer_backend pt \
  --attn_impl flash_attn \
  --stream true \
  --max_new_tokens 512
```

### 3.2 SGLang 后端

**特点**:
- ✅ 启动速度中等
- ✅ 并发性能好
- ✅ 支持多阶段推理

**适用场景**: 需要并发处理的推理服务、多消息推理

**命令示例**:
```bash
CUDA_VISIBLE_DEVICES=6 \
swift deploy \
  --model Qwen/Qwen2.5-7B-Instruct \
  --adapters ./output/mh-sft/v0-20250820-144506/checkpoint-508 \
  --infer_backend sglang \
  --served_model_name mh-sft-7b
```

### 3.3 LMDeploy 后端

**特点**:
- ✅ 启动速度快
- ✅ 推理性能好
- ✅ 显存利用率友好

**适用场景**: 生产环境推理服务、单卡推理优化

**命令示例**:
```bash
CUDA_VISIBLE_DEVICES=6 \
swift deploy \
  --model Qwen/Qwen2.5-7B-Instruct \
  --adapters ./output/mh-sft/v0-20250820-144506/checkpoint-508 \
  --infer_backend lmdeploy \
  --served_model_name mh-sft-7b \
  --tp 1 \
  --cache-max-entry-count 0 \
  --session-len 4096
```

### 3.4 vLLM 后端

**特点**:
- ⚠️ 启动速度较慢
- ✅ 吞吐量最高
- ✅ 支持连续批处理

**适用场景**: 高并发生产服务、需要高吞吐量的场景

## 4. 最佳实践

### 4.1 GPU 设备管理

**推荐做法**:
```bash
# 使用环境变量固定 GPU
CUDA_VISIBLE_DEVICES=6 swift infer ...

# 或在脚本中设置
export CUDA_VISIBLE_DEVICES=6
swift infer ...
```

**避免做法**:
```bash
# 不要使用不存在的参数
--device cuda:6  # ❌ 错误
--device_map cuda:6  # ✅ 正确
```

### 4.2 参数使用规范

**支持的参数**:
- `--model`: 基础模型名称
- `--adapters`: LoRA 检查点路径
- `--infer_backend`: 推理后端选择
- `--attn_impl`: 注意力实现方式
- `--stream`: 流式输出
- `--max_new_tokens`: 最大生成长度

**不支持的参数**:
- `--prompt`: 使用交互式输入或 Python API
- `--device`: 使用 `--device_map` 或环境变量

### 4.3 性能优化建议

1. **启动速度优化**:
   - 优先使用 `pt` 后端进行快速测试
   - 使用 `lmdeploy` 后端平衡启动速度和性能

2. **显存优化**:
   - 合理设置 `--max_new_tokens`
   - 使用 `--cache-max-entry-count 0` 动态分配 KV cache

3. **并发优化**:
   - 生产环境使用 `sglang` 或 `lmdeploy`
   - 高并发场景使用 `vLLM`

## 5. 常见问题排查

### 5.1 模型加载失败

**检查点**:
- 确认检查点路径正确
- 验证检查点完整性
- 检查基础模型是否可访问

### 5.2 推理质量异常

**检查点**:
- 确认使用了正确的 LoRA 检查点
- 验证训练数据质量
- 检查推理参数设置

### 5.3 性能问题

**检查点**:
- 确认 GPU 显存充足
- 验证 Flash Attention 是否正常工作
- 检查推理后端配置

## 6. 总结

通过本次推理测试，我们成功验证了：

1. ✅ LoRA 微调后的模型可以正常加载和推理
2. ✅ 多种推理后端都能正常工作
3. ✅ Flash Attention 集成成功
4. ✅ 中文输入输出问题已解决
5. ✅ 推理质量符合预期

**推荐配置**:
- 开发测试: 使用 `pt` 后端
- 生产服务: 使用 `lmdeploy` 后端
- 高并发: 使用 `sglang` 或 `vLLM` 后端

## 7. 变更日志

- 2025-08-20: 初始版本，记录推理测试过程和问题解决
- 2025-08-20: 添加推理后端对比和最佳实践
