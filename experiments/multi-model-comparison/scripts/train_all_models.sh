#!/bin/bash

# å¤šæ¨¡å‹æ‰¹é‡è®­ç»ƒè„šæœ¬
# ç”¨äºå¯¹æ¯”å¤šä¸ªå¼€æºæ¨¡å‹åœ¨å¿ƒç†å­¦ä»»åŠ¡ä¸Šçš„è¡¨ç°

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

# é…ç½®å˜é‡
export SWANLAB_TOKEN="P2PI8lAMWL1fF90kZAoXj"
export SWANLAB_PROJECT="multi-model-psychology"
export DATASET_ID="CodyWhy/mh-sharegpt-20250820"
export OUTPUT_BASE_DIR="./output"
export LOG_DIR="./logs"

# åˆ›å»ºå¿…è¦çš„ç›®å½•
mkdir -p $OUTPUT_BASE_DIR
mkdir -p $LOG_DIR

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆè§£å†³ä»£ç†é—®é¢˜ï¼‰
export NO_PROXY=127.0.0.1,localhost,::1
export no_proxy=$NO_PROXY

# æ¨¡å‹é…ç½®æ•°ç»„
declare -a models=(
    "qwen2.5-7b:Qwen/Qwen2.5-7B-Instruct:qwen2_5:qwen2_5"
    "llama3.1-8b:meta-llama/Meta-Llama-3.1-8B-Instruct:llama:llama3"
    "chatglm3-6b:ZhipuAI/chatglm3-6b:chatglm3:chatglm3"
    "internlm2-7b:internlm/internlm2-chat-7b:internlm2:internlm2"
    "baichuan2-7b:baichuan-inc/Baichuan2-7B-Chat:baichuan2:baichuan2"
)

# è®­ç»ƒå‡½æ•°
train_model() {
    local model_name=$1
    local model_id=$2
    local model_type=$3
    local template=$4
    
    echo "ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹: $model_name"
    echo "ğŸ“‹ æ¨¡å‹ID: $model_id"
    echo "ğŸ”§ æ¨¡å‹ç±»å‹: $model_type"
    echo "ğŸ“ æ¨¡æ¿: $template"
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    local output_dir="$OUTPUT_BASE_DIR/$model_name-sft"
    local log_file="$LOG_DIR/${model_name}_training.log"
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    mkdir -p $output_dir
    
    # æ ¹æ®æ¨¡å‹å¤§å°è°ƒæ•´æ‰¹æ¬¡å¤§å°
    local batch_size=8
    local grad_accum_steps=4
    
    if [[ $model_name == *"8b"* ]] || [[ $model_name == *"13b"* ]]; then
        batch_size=6
        grad_accum_steps=6
    fi
    
    echo "âš™ï¸ è®­ç»ƒå‚æ•°: batch_size=$batch_size, grad_accum_steps=$grad_accum_steps"
    
    # å¼€å§‹è®­ç»ƒ
    local start_time=$(date +%s)
    
    CUDA_VISIBLE_DEVICES=6 \
    swift sft \
        --model $model_id \
        --model_type $model_type \
        --template $template \
        --train_type lora \
        --dataset $DATASET_ID \
        --bf16 true \
        --max_length 3072 \
        --packing true \
        --gradient_checkpointing true \
        --per_device_train_batch_size $batch_size \
        --gradient_accumulation_steps $grad_accum_steps \
        --learning_rate 2e-4 \
        --num_train_epochs 1 \
        --warmup_ratio 0.1 \
        --save_steps 200 \
        --save_total_limit 3 \
        --logging_steps 20 \
        --output_dir $output_dir \
        --report_to swanlab \
        --swanlab_token $SWANLAB_TOKEN \
        --swanlab_project $SWANLAB_PROJECT \
        --swanlab_mode cloud \
        --swanlab_exp_name "${model_name}-sft-psychology" \
        --attn_impl flash_attn \
        2>&1 | tee $log_file
    
    local end_time=$(date +%s)
    local duration=$((end_time - start_time))
    
    echo "âœ… æ¨¡å‹ $model_name è®­ç»ƒå®Œæˆï¼"
    echo "â±ï¸ æ€»è€—æ—¶: $((duration / 60)) åˆ†é’Ÿ"
    echo "ğŸ“ è¾“å‡ºç›®å½•: $output_dir"
    echo "ğŸ“ æ—¥å¿—æ–‡ä»¶: $log_file"
    echo "---"
}

# ä¸»è®­ç»ƒå¾ªç¯
main() {
    echo "ğŸ¯ å¤šæ¨¡å‹å¿ƒç†å­¦ä»»åŠ¡è®­ç»ƒå¼€å§‹"
    echo "ğŸ“… å¼€å§‹æ—¶é—´: $(date)"
    echo "ğŸ”§ ä½¿ç”¨ GPU: CUDA_VISIBLE_DEVICES=6"
    echo "ğŸ“Š SwanLab é¡¹ç›®: $SWANLAB_PROJECT"
    echo "ğŸ“š æ•°æ®é›†: $DATASET_ID"
    echo "---"
    
    # è®°å½•å¼€å§‹æ—¶é—´
    local total_start_time=$(date +%s)
    
    # éå†æ‰€æœ‰æ¨¡å‹è¿›è¡Œè®­ç»ƒ
    for model_config in "${models[@]}"; do
        IFS=':' read -r model_name model_id model_type template <<< "$model_config"
        
        echo "ğŸ”„ å‡†å¤‡è®­ç»ƒæ¨¡å‹: $model_name"
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»è®­ç»ƒè¿‡
        if [ -d "$OUTPUT_BASE_DIR/$model_name-sft" ]; then
            echo "âš ï¸  æ¨¡å‹ $model_name å·²ç»å­˜åœ¨ï¼Œè·³è¿‡è®­ç»ƒ"
            echo "   å¦‚éœ€é‡æ–°è®­ç»ƒï¼Œè¯·åˆ é™¤ç›®å½•: $OUTPUT_BASE_DIR/$model_name-sft"
            echo "---"
            continue
        fi
        
        # å¼€å§‹è®­ç»ƒ
        train_model "$model_name" "$model_id" "$model_type" "$template"
        
        # è®­ç»ƒé—´éš”ï¼Œé¿å…èµ„æºå†²çª
        echo "â³ ç­‰å¾… 30 ç§’åç»§ç»­ä¸‹ä¸€ä¸ªæ¨¡å‹..."
        sleep 30
    done
    
    # è®¡ç®—æ€»è€—æ—¶
    local total_end_time=$(date +%s)
    local total_duration=$((total_end_time - total_start_time))
    
    echo "ğŸ‰ æ‰€æœ‰æ¨¡å‹è®­ç»ƒå®Œæˆï¼"
    echo "ğŸ“… ç»“æŸæ—¶é—´: $(date)"
    echo "â±ï¸ æ€»è€—æ—¶: $((total_duration / 3600)) å°æ—¶ $(((total_duration % 3600) / 60)) åˆ†é’Ÿ"
    echo "ğŸ“Š è®­ç»ƒç»“æœä¿å­˜åœ¨: $OUTPUT_BASE_DIR"
    echo "ğŸ“ è®­ç»ƒæ—¥å¿—ä¿å­˜åœ¨: $LOG_DIR"
    
    # ç”Ÿæˆè®­ç»ƒæ€»ç»“
    echo "ğŸ“‹ è®­ç»ƒæ€»ç»“:"
    for model_config in "${models[@]}"; do
        IFS=':' read -r model_name model_id model_type template <<< "$model_config"
        local output_dir="$OUTPUT_BASE_DIR/$model_name-sft"
        if [ -d "$output_dir" ]; then
            echo "  âœ… $model_name: $output_dir"
        else
            echo "  âŒ $model_name: è®­ç»ƒå¤±è´¥æˆ–æœªå®Œæˆ"
        fi
    done
}

# é”™è¯¯å¤„ç†
error_handler() {
    local exit_code=$?
    echo "âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼Œé€€å‡ºç : $exit_code"
    echo "ğŸ“ è¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶: $LOG_DIR"
    exit $exit_code
}

# è®¾ç½®é”™è¯¯å¤„ç†
trap error_handler ERR

# æ£€æŸ¥ä¾èµ–
check_dependencies() {
    echo "ğŸ” æ£€æŸ¥è®­ç»ƒç¯å¢ƒ..."
    
    # æ£€æŸ¥ swift å‘½ä»¤
    if ! command -v swift &> /dev/null; then
        echo "âŒ é”™è¯¯: æœªæ‰¾åˆ° swift å‘½ä»¤"
        echo "   è¯·å…ˆå®‰è£… ms-swift: pip install 'ms-swift[all]'"
        exit 1
    fi
    
    # æ£€æŸ¥ GPU
    if ! nvidia-smi &> /dev/null; then
        echo "âŒ é”™è¯¯: æœªæ‰¾åˆ° NVIDIA GPU"
        echo "   è¯·ç¡®ä¿åœ¨ GPU ç¯å¢ƒä¸­è¿è¡Œæ­¤è„šæœ¬"
        exit 1
    fi
    
    # æ£€æŸ¥æ˜¾å­˜
    local gpu_memory=$(nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits | head -1)
    if [ "$gpu_memory" -lt 70000 ]; then
        echo "âš ï¸  è­¦å‘Š: GPU æ˜¾å­˜å¯èƒ½ä¸è¶³ (å½“å‰: ${gpu_memory}MB, å»ºè®®: 80GB+)"
        echo "   å»ºè®®ä½¿ç”¨ H100 æˆ– A100 è¿›è¡Œè®­ç»ƒ"
    fi
    
    echo "âœ… ç¯å¢ƒæ£€æŸ¥é€šè¿‡"
    echo "---"
}

# è¿è¡Œä¸»å‡½æ•°
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    check_dependencies
    main "$@"
fi
