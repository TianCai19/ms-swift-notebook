# Llama3.1-8B-Instruct æ¨¡å‹é…ç½®

## ğŸ“‹ åŸºæœ¬ä¿¡æ¯

- **æ¨¡å‹åç§°**: Llama3.1-8B-Instruct
- **å‚æ•°é‡**: 8B
- **è®¸å¯è¯**: Meta License (éœ€è¦ç”³è¯·)
- **å‘å¸ƒæ–¹**: Meta AI
- **æ¨¡å‹ID**: meta-llama/Meta-Llama-3.1-8B-Instruct
- **ç‰¹ç‚¹**: Meta æœ€æ–°æ¨¡å‹ï¼Œæ€§èƒ½ä¼˜ç§€ï¼Œä½†ä¸­æ–‡éœ€è¦é¢å¤–ä¼˜åŒ–

## ğŸš€ æ¨¡å‹ä¼˜åŠ¿

1. **æ€§èƒ½ä¼˜ç§€**: Meta æœ€æ–°æŠ€æœ¯ï¼ŒåŸºç¡€èƒ½åŠ›å¼ºå¤§
2. **æ¶æ„å…ˆè¿›**: é‡‡ç”¨æœ€æ–°çš„ Transformer æ¶æ„
3. **å¤šè¯­è¨€**: æ”¯æŒå¤šç§è¯­è¨€ï¼ŒåŒ…æ‹¬ä¸­æ–‡
4. **å®‰å…¨æ€§**: ç»è¿‡å®‰å…¨å¯¹é½è®­ç»ƒ
5. **ç¤¾åŒºæ”¯æŒ**: åºå¤§çš„å¼€æºç¤¾åŒº

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **è®¸å¯è¯**: éœ€è¦ç”³è¯· Meta è®¸å¯è¯æ‰èƒ½å•†ç”¨
2. **ä¸­æ–‡èƒ½åŠ›**: åŸç”Ÿä¸­æ–‡èƒ½åŠ›æœ‰é™ï¼Œéœ€è¦å¾®è°ƒä¼˜åŒ–
3. **æ˜¾å­˜éœ€æ±‚**: 8B å‚æ•°ï¼Œæ˜¾å­˜éœ€æ±‚ç•¥é«˜äº 7B æ¨¡å‹

## âš™ï¸ è®­ç»ƒé…ç½®

### åŸºç¡€é…ç½®
```bash
# æ¨¡å‹åŠ è½½
--model meta-llama/Meta-Llama-3.1-8B-Instruct
--model_type llama
--template llama3

# è®­ç»ƒå‚æ•°
--train_type lora
--dataset CodyWhy/mh-sharegpt-20250820
--bf16 true
--max_length 3072
--packing true
--gradient_checkpointing true
```

### LoRA é…ç½®
```bash
# LoRA å‚æ•°
--lora_rank 8
--lora_alpha 32
--lora_dropout 0.1
--target_modules q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj
```

### è®­ç»ƒè¶…å‚æ•°
```bash
# æ‰¹æ¬¡å’Œä¼˜åŒ–
--per_device_train_batch_size 6  # 8B æ¨¡å‹ï¼Œæ‰¹æ¬¡ç¨å°
--gradient_accumulation_steps 6  # å¢åŠ æ¢¯åº¦ç´¯ç§¯
--learning_rate 2e-4
--num_train_epochs 1
--warmup_ratio 0.1

# ä¿å­˜å’Œæ—¥å¿—
--save_steps 200
--save_total_limit 3
--logging_steps 20
--report_to swanlab
```

### æ˜¾å­˜ä¼˜åŒ–
```bash
# Flash Attention
--attn_impl flash_attn

# æ¢¯åº¦ç´¯ç§¯
--gradient_accumulation_steps 6

# æ¢¯åº¦æ£€æŸ¥ç‚¹
--gradient_checkpointing true

# æ··åˆç²¾åº¦
--bf16 true
```

## ğŸ“Š é¢„æœŸæ€§èƒ½

### è®­ç»ƒé˜¶æ®µ
- **æ˜¾å­˜ä½¿ç”¨**: ~55-60GB (H100 80GB è¶³å¤Ÿ)
- **è®­ç»ƒé€Ÿåº¦**: ~2.5-3.5 å°æ—¶/epoch
- **æ”¶æ•›æ€§**: 1-2 epoch å³å¯æ”¶æ•›

### æ¨ç†é˜¶æ®µ
- **æ˜¾å­˜ä½¿ç”¨**: ~18-22GB
- **æ¨ç†é€Ÿåº¦**: ä¸­ç­‰ï¼Œé€‚åˆæ‰¹é‡æ¨ç†
- **è´¨é‡**: åŸºç¡€èƒ½åŠ›å¼ºï¼Œä¸­æ–‡éœ€è¦ä¼˜åŒ–

## ğŸ”§ éƒ¨ç½²é…ç½®

### LMDeploy éƒ¨ç½²
```bash
swift deploy \
  --model meta-llama/Meta-Llama-3.1-8B-Instruct \
  --adapters ./output/llama3.1-8b-sft/checkpoint-xxx \
  --infer_backend lmdeploy \
  --served_model_name llama3.1-8b-sft \
  --tp 1 \
  --cache-max-entry-count 0 \
  --session-len 4096
```

### æ¨¡å‹å¯¼å‡º
```bash
swift export \
  --ckpt_dir ./output/llama3.1-8b-sft/checkpoint-xxx \
  --merge_lora true \
  --safe_serialization true \
  --max_shard_size 2GB \
  --output_dir ./export/llama3.1-8b-sft
```

## ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡

### å®¢è§‚æŒ‡æ ‡
- **å‡†ç¡®ç‡**: åœ¨å¿ƒç†å­¦çŸ¥è¯†æµ‹è¯•ä¸Šçš„è¡¨ç°
- **BLEU/Rouge**: å¯¹è¯è´¨é‡è¯„ä¼°
- **å®‰å…¨æ€§**: æœ‰å®³å†…å®¹æ£€æµ‹

### ä¸»è§‚æŒ‡æ ‡
- **ä¸“ä¸šæ€§**: å¿ƒç†å­¦çŸ¥è¯†å‡†ç¡®æ€§
- **åŒç†å¿ƒ**: æƒ…æ„Ÿç†è§£å’Œæ”¯æŒèƒ½åŠ›
- **å®ç”¨æ€§**: å®é™…å’¨è¯¢åœºæ™¯çš„é€‚ç”¨æ€§
- **ä¸­æ–‡æµç•…åº¦**: ä¸­æ–‡è¡¨è¾¾çš„è‡ªç„¶ç¨‹åº¦

## ğŸ” ä¸­æ–‡ä¼˜åŒ–å»ºè®®

1. **æ•°æ®å¢å¼º**: å¢åŠ ä¸­æ–‡å¿ƒç†å­¦æ•°æ®
2. **æ¨¡æ¿ä¼˜åŒ–**: ä½¿ç”¨ä¸­æ–‡å‹å¥½çš„å¯¹è¯æ¨¡æ¿
3. **åå¤„ç†**: å¯¹ä¸­æ–‡è¾“å‡ºè¿›è¡Œè´¨é‡æ£€æŸ¥
4. **äººå·¥åé¦ˆ**: æ”¶é›†ä¸­æ–‡ä½¿ç”¨è€…çš„åé¦ˆ

## ğŸ“š å‚è€ƒèµ„æ–™

- [Llama3.1 å®˜æ–¹æ–‡æ¡£](https://github.com/meta-llama/llama3)
- [Meta AI æ¨¡å‹é¡µé¢](https://ai.meta.com/llama/)
- [ms-swift è®­ç»ƒæŒ‡å—](https://github.com/modelscope/swift)

---

*é…ç½®ç‰ˆæœ¬: v1.0*  
*æœ€åæ›´æ–°: 2025-08-20*
