# Qwen2.5-7B-Instruct æ¨¡å‹é…ç½®

## ğŸ“‹ åŸºæœ¬ä¿¡æ¯

- **æ¨¡å‹åç§°**: Qwen2.5-7B-Instruct
- **å‚æ•°é‡**: 7B
- **è®¸å¯è¯**: Apache 2.0
- **å‘å¸ƒæ–¹**: Alibaba Cloud
- **æ¨¡å‹ID**: Qwen/Qwen2.5-7B-Instruct
- **ç‰¹ç‚¹**: åŸç”Ÿä¸­æ–‡æ”¯æŒï¼Œå¯¹è¯èƒ½åŠ›å¼ºï¼Œå¿ƒç†å­¦ä»»åŠ¡è¡¨ç°ä¼˜ç§€

## ğŸš€ æ¨¡å‹ä¼˜åŠ¿

1. **ä¸­æ–‡èƒ½åŠ›**: åŸç”Ÿæ”¯æŒä¸­æ–‡ï¼Œæ— éœ€é¢å¤–å¾®è°ƒ
2. **å¯¹è¯è´¨é‡**: åœ¨ ShareGPT æ ¼å¼æ•°æ®ä¸Šè¡¨ç°ä¼˜ç§€
3. **å®‰å…¨æ€§**: å†…ç½®å®‰å…¨å¯¹é½ï¼Œé€‚åˆå¿ƒç†å’¨è¯¢åœºæ™¯
4. **æ•ˆç‡**: 7B å‚æ•°ï¼Œå• H100 è®­ç»ƒæ•ˆç‡é«˜
5. **ç¤¾åŒº**: é˜¿é‡Œäº‘ç»´æŠ¤ï¼Œæ›´æ–°é¢‘ç¹

## âš™ï¸ è®­ç»ƒé…ç½®

### åŸºç¡€é…ç½®
```bash
# æ¨¡å‹åŠ è½½
--model Qwen/Qwen2.5-7B-Instruct
--model_type qwen2_5
--template qwen2_5

# è®­ç»ƒå‚æ•°
--train_type lora
--dataset CodyWhy/mh-sharegpt-20250820
--bf16 true
--max_length 3072
--packing true
--gradient_checkpointing true
```

### LoRA é…ç½®
```bash
# LoRA å‚æ•°ï¼ˆms-swift é»˜è®¤ï¼‰
--lora_rank 8
--lora_alpha 32
--lora_dropout 0.1
--target_modules q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj
```

### è®­ç»ƒè¶…å‚æ•°
```bash
# æ‰¹æ¬¡å’Œä¼˜åŒ–
--per_device_train_batch_size 8
--gradient_accumulation_steps 4
--learning_rate 2e-4
--num_train_epochs 1
--warmup_ratio 0.1

# ä¿å­˜å’Œæ—¥å¿—
--save_steps 200
--save_total_limit 3
--logging_steps 20
--report_to swanlab
```

### æ˜¾å­˜ä¼˜åŒ–
```bash
# Flash Attention
--attn_impl flash_attn

# æ¢¯åº¦ç´¯ç§¯
--gradient_accumulation_steps 4

# æ¢¯åº¦æ£€æŸ¥ç‚¹
--gradient_checkpointing true

# æ··åˆç²¾åº¦
--bf16 true
```

## ğŸ“Š é¢„æœŸæ€§èƒ½

### è®­ç»ƒé˜¶æ®µ
- **æ˜¾å­˜ä½¿ç”¨**: ~45-50GB (H100 80GB è¶³å¤Ÿ)
- **è®­ç»ƒé€Ÿåº¦**: ~2-3 å°æ—¶/epoch
- **æ”¶æ•›æ€§**: 1-2 epoch å³å¯æ”¶æ•›

### æ¨ç†é˜¶æ®µ
- **æ˜¾å­˜ä½¿ç”¨**: ~15-20GB
- **æ¨ç†é€Ÿåº¦**: å¿«ï¼Œé€‚åˆå®æ—¶å¯¹è¯
- **è´¨é‡**: åœ¨å¿ƒç†å­¦ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜ç§€

## ğŸ”§ éƒ¨ç½²é…ç½®

### LMDeploy éƒ¨ç½²
```bash
swift deploy \
  --model Qwen/Qwen2.5-7B-Instruct \
  --adapters ./output/qwen2.5-7b-sft/checkpoint-xxx \
  --infer_backend lmdeploy \
  --served_model_name qwen2.5-7b-sft \
  --tp 1 \
  --cache-max-entry-count 0 \
  --session-len 4096
```

### æ¨¡å‹å¯¼å‡º
```bash
swift export \
  --ckpt_dir ./output/qwen2.5-7b-sft/checkpoint-xxx \
  --merge_lora true \
  --safe_serialization true \
  --max_shard_size 2GB \
  --output_dir ./export/qwen2.5-7b-sft
```

## ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡

### å®¢è§‚æŒ‡æ ‡
- **å‡†ç¡®ç‡**: åœ¨å¿ƒç†å­¦çŸ¥è¯†æµ‹è¯•ä¸Šçš„è¡¨ç°
- **BLEU/Rouge**: å¯¹è¯è´¨é‡è¯„ä¼°
- **å®‰å…¨æ€§**: æœ‰å®³å†…å®¹æ£€æµ‹

### ä¸»è§‚æŒ‡æ ‡
- **ä¸“ä¸šæ€§**: å¿ƒç†å­¦çŸ¥è¯†å‡†ç¡®æ€§
- **åŒç†å¿ƒ**: æƒ…æ„Ÿç†è§£å’Œæ”¯æŒèƒ½åŠ›
- **å®ç”¨æ€§**: å®é™…å’¨è¯¢åœºæ™¯çš„é€‚ç”¨æ€§

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **Flash Attention**: éœ€è¦æ­£ç¡®å®‰è£… flash_attn åŒ…
2. **ä¸­æ–‡ç¼–ç **: ç¡®ä¿ç³»ç»Ÿ locale è®¾ç½®æ­£ç¡®
3. **æ˜¾å­˜ç›‘æ§**: è®­ç»ƒè¿‡ç¨‹ä¸­ç›‘æ§æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
4. **æ¨¡å‹ä¿å­˜**: å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹ï¼Œé¿å…è®­ç»ƒä¸­æ–­

## ğŸ“š å‚è€ƒèµ„æ–™

- [Qwen2.5 å®˜æ–¹æ–‡æ¡£](https://github.com/QwenLM/Qwen2)
- [ModelScope æ¨¡å‹é¡µé¢](https://modelscope.cn/models/Qwen/Qwen2.5-7B-Instruct)
- [ms-swift è®­ç»ƒæŒ‡å—](https://github.com/modelscope/swift)

---

*é…ç½®ç‰ˆæœ¬: v1.0*  
*æœ€åæ›´æ–°: 2025-08-20*
